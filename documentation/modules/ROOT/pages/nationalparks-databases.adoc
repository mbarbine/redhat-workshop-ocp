= Connecting to a Database
:navtitle: Connecting to a Database

In this section we will deploy and connect a MongoDB database where the
`nationalparks` application will store location information.

Finally, we will mark the `nationalparks` application as a backend for the map
visualization tool, so that it can be dynamically discovered by the `parksmap`
component using the OpenShift discovery mechanism and the map will be displayed
automatically.

image::roadshow-app-architecture-nationalparks-2.png[Application architecture,800,align="center"]

[#storage]
== Background: Storage

Most useful applications are "stateful" or "dynamic" in some way, and this is
usually achieved with a database or other data storage. In this lab we are
going to add MongoDB to our `nationalparks` application and then rewire it to
talk to the database using environment variables via a secret.

// We'll take two approaches, using MongoDB Atlas (a fully-managed cloud database), as well as a containerized package of MongoDB.

[#create_mongodb_instance]
== Exercise: Deploy MongoDB

////
[tabs, subs="attributes+,+macros"]
====

MongoDB Atlas::
+
--
OpenShift provides support for https://www.mongodb.com/atlas/database[MongoDB Atlas], as well as various other 3rd party database services using OpenShift Database Access (RHODA).

The first step in using the Atlas Operator is to provide it with the appropriate permissions from your Atlas account.
If you haven't yet already, https://www.mongodb.com/cloud/atlas/register[create a MongoDB Atlas account] or https://account.mongodb.com/account/login[login] and navigate to your organization settings by clicking the **Gear** icon from your Atlas homepage.

image::mongodb-atlas-settings.png[MongoDB Atlas Settings]

Here, copy the Organization ID, and save it somewhere for future usage.

Back in MongoDB Atlas, you'll also need to create an API key with the appropriate permissions.
In the left hand tab, select **Access Manager**, and click **Create API Key**

image::mongodb-atlas-access-manager.png[MongoDB Atlas Access Manager]

For the API Key, choose any name, and ensure the Key has **Organization Owner & Organization Project Creator** permissions before continuing.

image::mongodb-atlas-api-key.png[MongoDB Atlas API Key]

Here, save your public and private keys to for future usage, as we'll be using the RHODA operator with these saved credentials.

image::mongodb-atlas-api-key-details.png[MongoDB Atlas API Key Details]

.Using the RHODA operator
****
From the left navigation bar on the **Developer** perspective, click on **+Add**. Click on the "All Services" card under the "Developer Catalog" section.

This will open up the *Developer Catalog*. In the search bar, enter "Provider Account", and scroll down to find the "Provider Account" card. Click on it to open the side panel, and click on "Create".

image::openshift-provider-account.png[OpenShift Provider Account]

If you don't already happen to have a "Provider Account Policy", select the **create a Policy** link to create a DBaaSPolicy. Feel free to use a name such as `openshift-mongodb-policy`, and click **Create**.

image::dbaas-policy.png[OpenShift DBaaS Policy Creation]

From here, using the left navigation bar, click on *+Add*. Return to the *Developer Catalog*, click "All Services" and again search for "Provider Account", this time being greeted by a page to enter our previously saved values.

image::import-provider-account.png[Import Database Provider Account]

Hit **Import**. If you have clusters already provisioned you should see a success message, along with a list of all of your MongoDB projects. If none were provisioned you'll see an error message that no instances were found.

image::databases-fetched.png[Fetched Databases]
****

From the **+Add** menu again, go to "All Services" in "Developer Catalog", and seach for "Atlas". Scroll down and pick "MongoDB Atlas Cloud Database Service". Click on *Add to Topology*.

image::databases-fetched.png[MongoDB Atlas Cloud Database Service]

TIP: If you don't want to create a new database, you should be able to use the one you created with the RHODA operator.

From the Provider Account dropdown, pick openshift-mongodb. Then, click on the *Create New Database Instance* on the right side, just above the list of existing databases.

image::atlas-create-new-database.png[MongoDB Atlas Create New Database]

In the "Create New Instance" form, use the following values.

*Database Provider:* MongoDB Atlas Cloud Database Service Provider

*Account:* openshift-mongodb

*Instance Name:*

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
mongodb
----

*Project Name:*

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
mongodb-nationalparks
----

image::atlas-new-instance.png[MongoDB Atlas Create New Instance]

Once finished and you've clicked **Create**, in Atlas you should now see a new project called `mongodb-nationalparks` (you might need to refresh the page). If you select that project, you will see a new cluster called `mongodb` being created.

You'll also need to allow all IP access on MongoDB Atlas so our pods can access Atlas. For a production setting, however, you would either set up a PrivateLink or add the IPs of your OpenShift hosts.

image::atlas-allow-ip.png[MongoDB Atlas Allow All IP's]

Finally, from the *+Add* menu, go to "All Services" in "Developer Catalog", seach for "Atlas", and pick "MongoDB Atlas Cloud Database Service". Click on *Add to Topology*.

image::openshift-developer-catalog-mongodb.png[OpenShift Developer Catalog MongoDB]

Now, let's specify these values to the MongoDB Atlas Operator, and create a secret. 

In the Provider Account dropdown, pick *openshift-mongodb*. From the list of available database instances, pick the newly created `mongodb`. Click on *Add to Topology*.

image::openshift-mongodb-add-topology.png[OpenShift MongoDB Add To Topology]

From the Topology view, you should now see the Database as a Service Connection (DBSC). 
Hover the "nationalparks" icon. You will see a dotted arrow. Hover that arrow, then drag and drop it inside the square area of the DBSC (just the grey area, not the icon inside of it). A modal will popup, click on *Create* to create the service bindings.

image::openshift-service-binding.png[OpenShift Service Binding]

The application will redeploy with the new service bindings. After successfully creating the service binding application will be connected to the database.
--

MongoDB Community Image::
+
--
////
The first step is to create Kubernetes **Secrets** which are used to store database-related configuration and credentials. In the Developer perspective, choose Secrets in the left navigation menu, then click the **Create** button in the upper right and select **Key/value secret**.

image::mongodb_create_secret.png[MongoDB Create K/V Secret]
 
In the dialog, fill in the entries shown below, by **adding four key/value pairs** in total.

* Secret Name -> `mongodb-credentials`
    - key: `admin-usr` | value: `admin`
    - key: `admin-pwd` | value: `secret`
    - key: `app-usr` | value: `parksapp`
    - key: `app-pwd` | value: `keepsafe`

image::mongodb_create_secret_dialog.png[MongoDB Create K/V Secret]

Click the **Create** button at the bottom of the form.

With the secret in place, click on **+Add** from the left navigation bar on the **Developer** perspective. Then choose the tile for adding **Container images** which will open up the *Deploy Image* dialog. Fill in the following details:

* Image section:
    - Image name from external registry -> `mongo:6.0.4`
    - Runtime icon -> choose `mongodb`

image::mongodb_deploy_image_image.png[MongoDB Image Deployment Image Setting]

* General section:
    - Application -> `workshop`
    - Name -> `mongodb`

image::mongodb_deploy_image_general.png[MongoDB Image Deployment General Settings]

* Advanced options:
    - Create route -> *uncheck* because no route is needed to the DB instance itself

image::mongodb_deploy_image_advanced.png[MongoDB Image Deployment Advanced Settings]

Click on **Resource type** to show its settings.

image::mongodb_deploy_image_option_resource_type.png[MongoDB Image Deployment Resource Type Settings]

Make sure that the drop-down for resource type is set to the **Deployment** option.

image::mongodb_deploy_image_resource_type.png[MongoDB Image Deployment Resource Type Settings]

Click on **Deployment** to show its settings.

image::mongodb_deploy_image_option_deployment.png[MongoDB Image Deployment Deployment Settings]

Here, we'll add the following two environment variables which will be sourced from the previously created secret named `mongodb-credentials`.

Click *+Add from ConfigMap or Secret* and enter `MONGO_INITDB_ROOT_USERNAME` as the name. For the value, choose `mongodb-credentials` as resource and `admin-usr` as key.

Again, click *+Add from ConfigMap or Secret* and enter `MONGO_INITDB_ROOT_PASSWORD` as the name. For the value, choose `mongodb-credentials` as resource and `admin-pwd` as key.

image::mongodb_deploy_image_deployment_env_vars_secrets.png[MongoDB Image Deployment Environment Variable Settings]

Finally click the **Create** button at the bottom of the page to deploy MongoDB into your topology.

The next step is to create a specific user in this MongoDB instance which the nationalparks application will use to create a connection. For this click the **MongoDB deployment** in your topology view, then click on the corresponding **pod** shown in the right view pane.

image::mongodb_deployment_topology_pod.png[MongoDB Deployment Pod]

You will end up in the pod details view where you can click **Terminal** to start a terminal session in the underlying container. 

image::mongodb_pod_details_terminal.png[MongoDB Deployment Pod Details Terminal]

Copy and paste the following snippet into the terminal window to create a database user with the proper settings and roles:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
mongosh -u admin -p secret --authenticationDatabase admin --eval 'use parksapp' --eval 'db.createUser({user: "parksapp", pwd: "keepsafe", roles: [{ role: "dbAdmin", db: "parksapp" },{ role: "readWrite", db: "parksapp" }]})' --quiet
----

If the command is successful you'll see the output below:

[.console-output]
[source,bash,subs="+attributes,macros+"]
----
{ ok: 1 }
----

With that, everything regarding the database is prepared and we can switch our focus back to the nationalparks application. Go back to the **Topology View**, click the **nationalparks deployment** and choose _Actions > Edit Deployment_ from the drop-down in the right view pane. 

image::nationalparks_deployment_edit.png[Edit Nationalparks Deployment]

In the **Edit Deployment** dialog, scroll down to the **Environment Variables** section and add the following four entries. First set the server host and the database:

    - `MONGODB_SERVER_HOST`: `mongodb`
    - `MONGODB_DATABASE`: `parksapp`

The other two are credentials which are sourced from the `mongodb-credentials` secret.

Click *+Add from ConfigMap or Secret* and enter `MONGODB_USER` as the name. For the value, choose `mongodb-credentials` as resource and `app-usr` as key.

Again, click *+Add from ConfigMap or Secret* and enter `MONGODB_PASSWORD` as the name. For the value, choose `mongodb-credentials` as resource and `app-pwd` as key.

image::nationalparks_deployment_env_vars_secrets.png[Nationalparks Deployment Env Vars Secrets]

Finally hit the *Save* button at the bottom of the dialog. This will trigger a re-creation of a new pod and restart the application which should now be able to successfully talk to the MongoDB instance you just deployed earlier.

[#adding_labels]
== Exercise: Adding Labels

Next, let's fix the labels assigned to the MongoDB deployment. From the **Topology** view, select the MongoDB deployment and choose _Actions > Edit Labels_.

image::mongodb_deployment_labels.png[MongoDB Deployment Labels Option]

Like before, we'll add 3 labels:

The name of the Application group:

[source,role=copypaste]
----
app=workshop
----

Next the name of this deployment.

[source,role=copypaste]
----
component=nationalparks
----

And finally, the role this component plays in the overall application.

[source,role=copypaste]
----
role=database
----

image::mongodb_deployment_labels_save.png[MongoDB Deployment Labels Save]

[#exploring_openshift_magic]
== Exercise: Exploring OpenShift Magic

As soon as we connected our database, some
magic happened. OpenShift decided that this was a significant enough change to
warrant updating the internal version number of the *ReplicaSet*. You
can verify this by looking at the output of `oc get rs`:

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
NAME                       DESIRED   CURRENT   READY   AGE
nationalparks-58bd4758fc   0         0         0       4m58s
nationalparks-7445576cd9   0         0         0       6m42s
nationalparks-789c6bc4f4   1         1         1       41s
parksmap-57df75c46d        1         1         1       8m24s
parksmap-65c4f8b676        0         0         0       18m
----

We see that the DESIRED and CURRENT number of instances for the current deployment. The desired and current number of the other instances are 0.
This means that OpenShift has gracefully torn down our "old" application and
stood up a "new" instance.



[#data_data_everywhere]
== Exercise: Data, Data, Everywhere

Now that we have a database deployed, we can again visit the `nationalparks` web
service to query for data:

[source,role="copypaste",subs="+attributes"]
----
https://nationalparks-%PROJECT%.%CLUSTER_SUBDOMAIN%/ws/data/all
----

And the result?

[.console-output]
[source,bash]
----
[]
----

Where's the data? Think about the process you went through. You deployed the
application and then deployed the database. Nothing actually loaded anything
*INTO* the database, though.

The application provides an endpoint to do just that:

[source,role="copypaste",subs="+attributes"]
----
https://nationalparks-%PROJECT%.%CLUSTER_SUBDOMAIN%/ws/data/load
----

And the result?

[.console-output]
[source,bash]
----
Items inserted in database: 2893
----

If you then go back to `/ws/data/all` you will see tons of JSON data now.
That's great. Our parks map should finally work!

NOTE: There are some errors reported with browsers like Firefox 54 that don't properly parse the resulting JSON. It's
a browser problem, and the application is working properly.

[source,role="copypaste",subs="+attributes"]
----
https://parksmap-%PROJECT%.%CLUSTER_SUBDOMAIN%
----

Hmm... There's just one thing. The main map **STILL** isn't displaying the parks.
That's because the front end parks map only tries to talk to services that have
the right *Label*.

[NOTE]
====
You are probably wondering how the database connection magically started
working? When deploying applications to OpenShift, it is always best to use
environment variables, secrets, or configMaps to define connections to dependent systems.  This allows
for application portability across different environments.  The source file that
performs the connection as well as creates the database schema can be viewed
here:


[source,role="copypaste"]
----
http://www.github.com/openshift-roadshow/nationalparks/blob/master/src/main/java/com/openshift/evg/roadshow/parks/db/MongoDBConnection.java#L44-l48
----

In short summary: By referring to bindings to connect to services
(like databases), it can be trivial to promote applications throughout different
lifecycle environments on OpenShift without having to modify application code.

====

[#working_with_labels]
== Exercise: Working With Labels

We explored how a *Label* is just a key=value pair earlier when looking at
*Services* and *Routes* and *Selectors*. In general, a *Label* is simply an
arbitrary key=value pair. It could be anything.

* `pizza=pepperoni`
* `pet=dog`
* `openshift=awesome`

In the case of the parks map, the application is actually querying the OpenShift
API and asking about the *Routes* and *Services* in the project. If any of them have a
*Label* that is `type=parksmap-backend`, the application knows to interrogate
the endpoints to look for map data.
You can see the code that does this
link:https://github.com/openshift-roadshow/parksmap-web/blob/master/src/main/java/com/openshift/evg/roadshow/rest/RouteWatcher.java#L20[here].


Fortunately, the command line provides a convenient way for us to manipulate
labels. `describe` the `nationalparks` service:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc describe route nationalparks
----

[.console-output]
[source,bash,subs="+attributes,macros+"]
----
Name:                   nationalparks
Namespace:              %PROJECT%
Created:                2 hours ago
Labels:                 app=workshop
                        app.kubernetes.io/component=nationalparks
                        app.kubernetes.io/instance=nationalparks
                        app.kubernetes.io/name=java
                        app.kubernetes.io/part-of=workshop
                        app.openshift.io/runtime=java
                        app.openshift.io/runtime-version=11
                        component=nationalparks
                        role=backend  
Annotations:            openshift.io/host.generated=true                          
Requested Host:         nationalparks-%PROJECT%.%CLUSTER_SUBDOMAIN%
                        exposed on router router 2 hours ago
Path:                   <none>
TLS Termination:        <none>
Insecure Policy:        <none>
Endpoint Port:          8080-tcp

Service:                nationalparks
Weight:                 100 (100%)
Endpoints:              10.1.9.8:8080
----

You see that it already has some labels. Now, use `oc label`:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
oc label route nationalparks type=parksmap-backend
----

You will see something like:

[.console-output]
[source,bash]
----
route.route.openshift.io/nationalparks labeled
----

If you check your browser now:

[source,role="copypaste",subs="+attributes"]
----
https://parksmap-%PROJECT%.%CLUSTER_SUBDOMAIN%/
----

image::nationalparks-databases-new-parks.png[MongoDB]

You'll notice that the parks suddenly are showing up. That's really cool!
